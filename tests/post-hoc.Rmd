---
title: "Post hoc inference for multiple two-sample tests"
description: |
  A new article created using the Radix format.
author:
  - name: Pierre Neuvial
    url: https://math.univ-toulouse.fr/~pneuvial
    affiliation: Institut de Math√©matiques de Toulouse
    affiliation_url: https://math.univ-toulouse.fr
date: "`r Sys.Date()`"
output: radix::radix_article
header-includes:
- \newcommand{\cH}{\mathcal{H}}
- \renewcommand{\P}{\mathbb{P}}
- \newcommand{\oV}{\overline{V}}
bibliography: sansSouci.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Context

# Post hoc inference

Post hoc inference has been introduced by @GS2011. It builds on multiple testing theory to build constructing confidence bounds on arbitrary subsets of hypotheses. Formally, let $\cH$ be a set of $m$ null hypotheses, and $\cH_0$ be the (unknown) subset of true null hypotheses. Then for $S \subset \cH$, $|S \cap \cH_0|$ is the number of false positives in $S$. With this notation, $\oV$ is a post hoc upper bound at confidence level $\alpha$ if

$$\P(\forall S \subset \cH,  \quad |S \cap \cH_0| \leq \oV(S)) \geq 1-\alpha$$
That is, there exists an event of probability $1-\alpha$ such that *for any subset $S$ of hypotheses* -possibly data-driven or cherry-picked by a user-, the number of false positives in $S$ is less than $V(S)$.

**[TODO: By-product: Confidence envelopes from the FDP]**

This may seem an ambitious goal. @GS2011 have proposed a general framework based on closed testing in order to build such bounds. In particular, they provide such a bound in the case where the tested hypotheses satisfy a classical positive dependence assumption called PRDS (see @sarkar98probability for a formal definition). This work has been extended by @blanchard:posthoc, who showed that such post hoc bounds can be obtained as a consequence of the control of a multiple testing risk called the Joint Error Rate (JER). In particular, under PRDS, they recover the bound of @GS2011 under PRDS as a corollary of the Simes inequality ( @simes86improved), a probabilistic inequality which plays an important role in multiple testing. @simes86improved. 

An intrinsic limitation of post hoc bounds based on such a probabilistic control is that it is bound to *assume* a particular form of dependence to hold. As discussed in @blanchard:posthoc, this dependency may not always hold, and when it holds, the resulting bound may be overly conservative. To address this issue, @blanchard:posthoc have shown how JER control (and subsequent post hoc bounds) can be obtained under a generic randomization hypothesis. The goal of this note is to show how this randomization principle can be used in practice in the case of two-sample tests. 

### Permutation-based JER control for multiple two-sample tests

**[TODO:description of the methods]**

The methods discussed above are implemented in the R package `sansSouci`.  The methods of @GS2011 are implemented in the R package `cherry`.

```{r sanssouci}
library("sansSouci")
library("cherry")
```



## A multiple two-sample testing problem

### Data

Differential gene expression between two cancer subtypes. TODO: more thorough description.

- Link to the data (to be placed in a 'data' folder)

https://plmbox.math.cnrs.fr/f/755496cc4c154a6dbab0/?dl=1


```{r}
dat <- readRDS("../data/bourgon.rds")
```

The data consists of gene expression measurements obtained by micorrays:
    - $m = `r nrow(dat)`$ genes
    - $n = `r ncol(dat)`$ cancer patients


```{r}
table(colnames(dat))
```


### Classical differential analysis

We start with a simple Welch test for differential expression for each gene. (This can be done quickly using the `sansSouci::rowWelchTests` function) and a histogram of the corresponding $p$-values.

```{r}
rwt <- rowWelchTests(dat, colnames(dat))
pval <- rwt$p.value
hist(pval)
```

#### Multiple testing correction: False Discovery Rate control


```{r}
alpha <- 0.1
adjp.BH <- p.adjust(pval, method = "BH")
nBH <- sum(adjp.BH <= alpha)
```

```{r bonferroni, eval=FALSE, echo=FALSE}
adjp.Bonf <- p.adjust(pval, method = "bonferroni")
nB <- sum(adjp.Bonf <= alpha)
```



In this data set, `r nBH` genes are called differentially expressed at a False Discovery Rate (FDR) of $\alpha = `r alpha`$.




## Analysis using post hoc bounds

### 1 - Confidence envelopes

```{r}
stat <- rwt$statistic
m <- length(stat)
o <- order(stat, decreasing = TRUE)
thr <- SimesThresholdFamily(m)(alpha)
ub <- curveMaxFP(stat[o], thr)
K <- 50
plot(1:K, 1:K - ub[1:K], t = 's', col = "purple",
     ylab = "Lower bound on the number of false positives in selection")
abline(a = 0, b = 1, lty = 2)
```

**TODO: interactive plot with gene names**


### 2 - (basic) User interaction (TODO!)

**TODO: shiny app within radix document**

Example for Radix integration avaliable here https://github.com/rstudio/radix/blob/master/inst/examples/radix-interactive-document.Rmd
but I can't get it to work yet.

To run the shiny app in a standalone document, do

```{r, eval=FALSE}
runApp('tests/post-hoc/volcano-plot.R')
```


### 3 - Data-driven sets of hypotheses

Here we apply logistic lasso and elastic net regression in order to identify a subset of genes that can predict the "BCR/ABL" vs "NEG" status of a patient from their expression level:

```{r}
library("glmnet")
y <- colnames(dat)
x <- t(dat)
fitL <- glmnet(x, y, alpha = 1, family = "binomial")
fitE <- glmnet(x, y, alpha = 0.5, family = "binomial")
```


```{r}
plot(fitL, main = "Lasso regularization path")
```

```{r}
plot(fitE, main = "Elastic net regularization path")
```

```{r}
RSbar <- function(pred, fit) {
  beta <- fit$beta
  non0 <- colSums(beta != 0)
  ww <- max(which(non0 <= pred))
  ids <- which(beta[, ww] != 0)
  Sbar <- posthocBySimes(pval, select = ids, alpha = 0.05)
  c(R = length(ids), Sbar = Sbar) #, FDPbar = (R - Sbar)/max(R, 1) 
}
```


```{r}
npreds <- 1:50

stats <- sapply(npreds, RSbar, fitL)
stats <- data.frame(Sbar = stats["Sbar", ], R = stats["R", ])
stats$FDPbar <- (stats$R-stats$Sbar)/pmax(stats$R, 1)
statsL <- stats

stats <- sapply(npreds, RSbar, fitE)
stats <- data.frame(Sbar = stats["Sbar", ], R = stats["R", ])
stats$FDPbar <- (stats$R-stats$Sbar)/pmax(stats$R, 1)
statsE <- stats
```


We plot our obtained lower bound on the number of true positives as a function of the number of genes in the predictor:


```{r}
plot(statsE[["R"]], statsE[["Sbar"]], t = "b", 
     xlab = "Number of variables selected",
     ylab = "Lower bound on the number of true positives")
lines(statsL[["R"]], statsL[["Sbar"]], t = "b", col = 2)

legend("topleft", c("Elastic net (0.5)", "Lasso"), col = c(1,2), lty = 1, pch = 1)
```

With the same data we can plot the corresponding upper bound on the False Discovery Proportion:

```{r}
plot(stats[["R"]], stats[["FDPbar"]], t = "b", col = 2,
     xlab = "Number of variables selected by lasso",
     ylab = "Upper bound on the proportion of false positives")
lines(statsL[["R"]], statsL[["FDPbar"]], t = "b", col = 1)
legend("bottomright", c("Elastic net (0.5)", "Lasso"), col = c(1,2), lty = 1, pch = 1)
```

Importantly, all these bounds are valid *simultaneously*. 

TODO: choose the `alpha` parameter in elastic net *post hoc* using the above FDP curves!


## Session information

```{r session-info}
sessionInfo()
```
