---
title: "Post hoc inference for multiple two-sample tests"
description: |
    An example of methodological contribution with some user interaction.
author:
  - name: Pierre Neuvial
    url: https://math.univ-toulouse.fr/~pneuvial
    affiliation: Institut de Math√©matiques de Toulouse
    affiliation_url: https://math.univ-toulouse.fr
date: "`r Sys.Date()`"
runtime: shiny_prerendered
output: 
  radix::radix_article:
    toc: true
    toc_depth: 3
header-includes:
- \newcommand{\cH}{\mathcal{H}}
- \renewcommand{\P}{\mathbb{P}}
- \newcommand{\oV}{\overline{V}}
bibliography: sansSouci.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Context

### Post hoc inference

Post hoc inference has been introduced by @GS2011. It builds on multiple testing theory to build constructing confidence bounds on arbitrary subsets of hypotheses. Formally, let $\cH$ be a set of $m$ null hypotheses, and $\cH_0$ be the (unknown) subset of true null hypotheses. Then for $S \subset \cH$, $|S \cap \cH_0|$ is the number of false positives in $S$. With this notation, $\oV$ is a post hoc upper bound at confidence level $\alpha$ if

$$\P(\forall S \subset \cH,  \quad |S \cap \cH_0| \leq \oV(S)) \geq 1-\alpha$$
That is, there exists an event of probability $1-\alpha$ such that *for any subset $S$ of hypotheses* -possibly data-driven or cherry-picked by a user-, the number of false positives in $S$ is less than $V(S)$.

**[TODO: By-product: Confidence envelopes from the FDP]**

This may seem an ambitious goal. @GS2011 have proposed a general framework based on closed testing in order to build such bounds. In particular, they provide such a bound in the case where the tested hypotheses satisfy a classical positive dependence assumption called PRDS (see @sarkar98probability for a formal definition). This work has been extended by @blanchard:posthoc, who showed that such post hoc bounds can be obtained as a consequence of the control of a multiple testing risk called the Joint Error Rate (JER). In particular, under PRDS, they recover the bound of @GS2011 under PRDS as a corollary of the Simes inequality ( @simes86improved), a probabilistic inequality which plays an important role in multiple testing. @simes86improved. 

An intrinsic limitation of post hoc bounds based on such a probabilistic control is that it is bound to *assume* a particular form of dependence to hold. As discussed in @blanchard:posthoc, this dependency may not always hold, and when it holds, the resulting bound may be overly conservative. To address this issue, @blanchard:posthoc have shown how JER control (and subsequent post hoc bounds) can be obtained under a generic randomization hypothesis. The goal of this note is twofold:

- to illustrate the general interest of such post-hoc bounds, by showing that they make it possible to 
- to show how the above-mentioned randomization principle can be used in practice to get more powerful bounds in the specific case of two-sample tests for differential gene expression studies. 

### Permutation-based JER control for multiple two-sample tests

**[TODO:description of the methods]**

The methods discussed above are implemented in the R package `sansSouci`.  The methods of @GS2011 are implemented in the R package `cherry`.

```{r sanssouci}
library("sansSouci")
library("cherry")
```

## A multiple two-sample testing problem

### Data

Differential gene expression between two cancer subtypes. **TODO: more thorough description.**

- Link to the data (to be placed in a 'data' folder)

https://plmbox.math.cnrs.fr/f/755496cc4c154a6dbab0/?dl=1


```{r}
dat <- readRDS("../data/bourgon.rds")
```

The data consists of gene expression measurements obtained by micorrays:
- $m = `r nrow(dat)`$ genes
- $n = `r ncol(dat)`$ cancer patients


```{r}
table(colnames(dat))
```


### Classical differential analysis

We start with a simple Welch test for differential expression for each gene. This can be done e.g. using the `sansSouci::rowWelchTests` function:

```{r}
rwt <- rowWelchTests(dat, colnames(dat))
class(rwt) <- "list"
dex <- as.data.frame(rwt)
rm(rwt)
pval <- dex[["p.value"]]
```

We plot a histogram of the corresponding $p$-values:

```{r hist-ui, echo=FALSE}
inputPanel(
  selectInput("n_breaks", label = "Number of bins:",
              choices = c(10, 20, 35, 50), selected = 20)
)

plotOutput("hist")
```


```{r, hist-server, context="server"}
output$hist <- renderPlot({
  hist(dex$p.value, probability = TRUE, breaks = as.numeric(input$n_breaks),
       xlab = "p-value", main = "p-value distributon")
})
```

As expected, the distribution presents a large number of small $p$-values (which include signals, i.e. differentially expressed genes) mixed with uniformly distributed $p$-values (corresponding to non-differentially expressed genes).

#### Multiple testing correction: False Discovery Rate control

The state of the art approach to large-scale multiple testing is to control the False Discovery Rate (FDR), which is the expected proportion of wrongly selected genes (false positives) among all selected genes @benjamini95controlling. The most widely used method to control this risk is the Benjamini-Hochberg (BH) procedure, which has been shown to control the FDR when the hypotheses corresponding to the non-differentially expressed genes are independent @benjamini95controlling or satisfy the PRDS assumption @benjamini01control.

```{r bh}
alpha <- 0.1
adjp.BH <- p.adjust(pval, method = "BH")
nBH <- sum(adjp.BH <= alpha)
dex$adjp.BH <- adjp.BH
```

```{r, layout="l-body-outset"}
library("rmarkdown")
ww <- which(adjp.BH <= alpha)
o <- order(dex[["p.value"]])
odex <- dex[head(o, nBH), ]
paged_table(odex, options = list(rows.print = 20))
```


#### Caveat: interpretation of FDR control

In this data set, `r nBH` genes are called differentially expressed at a False Discovery Rate (FDR) of $\alpha = `r alpha`$. Importantly, this does not mean that the proportion of false positives in this list, which is called the FDP for False Discovery Proportion, is less than `r nBH*alpha`. Indeed, the FDP is a *random* quantity, so we can only have a probabilistic control on this quantity. The FDR is the *expected* FDP, that is, the average FDP over hypothetical replications of the same genomic experiment and $p$-value thresholding procedure. 


## Analysis using post hoc bounds

### 1 - Confidence envelopes

```{r}
stat <- dex[["statistic"]]
m <- length(stat)
o <- order(stat, decreasing = TRUE)
thr <- SimesThresholdFamily(m)(alpha)
ub <- curveMaxFP(stat[o], thr)
K <- 50
plot(1:K, 1:K - ub[1:K], t = 's', col = "purple",
     ylab = "Lower bound on the number of false positives in selection")
abline(a = 0, b = 1, lty = 2)
```

**TODO: interactive plot with gene names listed**


### 2 - User interaction

The usefulness of post hoc bounds for interactive data analysis can be illustrated by the volcano plot below. In this plot, each gene is represented as a data point in the (group difference, signficance space). More precisely, we plot the $p$-value of each gene (using a $-log_10$ scale, so that interesting genes are at the top), as a function of the difference between the mean expression level in both groups.

Below, the default selection consists of the  `r nBH` genes are called differentially expressed at a False Discovery Rate (FDR) of $\alpha = `r alpha`$. If one is interested 

```{r volcano-prep}
library("plotly")
dex[["logp"]] <- log10(dex[["p.value"]])
datly <- subset(dex, (abs(meanDiff) > 0.3) | (p.value < 0.01))
datly <- round(datly, 2)
datly[["id"]] <- rownames(datly)
```

```{r volcano-ui, echo=FALSE}
wellPanel(h2("Volcano plot"),
          plotlyOutput("plot"))
wellPanel(#h2("Gene selection"),
          h3("Confidence statement"),
          textOutput("bound"))
          # h3("List"),
          # dataTableOutput("selection")

```

```{r volcano-server, context="server"}
output$plot <- renderPlotly({
  rg <- range(datly$meanDiff)
  rg <- max(abs(rg))*c(-1,1)
  
  d <- event_data("plotly_selected")
#  print(d)
  if (is.null(d)) {
    mm <- which(datly[["adjp.BH"]] <= alpha)
  } else {
    mm <- match(d$key, datly[["id"]])
  }
#  print(mm)
  selected <- numeric(nrow(datly))
  selected[mm] <- 1
  datly$selected <- factor(selected)
  
  library("ggplot2")
  p <- ggplot(datly, aes(x = meanDiff, y = -logp, 
                         colour = selected, key = id))
  p <- p + geom_point(alpha = 0.2) +
    xlim(range(datly$meanDiff)) + ylim(c(0, max(-datly$logp))) +
    scale_colour_discrete(name = "", breaks = c("0", "1"),
                          labels = c("not selected", "selected"))  
  ggplotly(p)  %>% 
    config(displayModeBar = F) %>%
    layout(dragmode = "select",
           xaxis = list(range = rg))
}) 

output$bound <- renderText({
  msg <- sprintf("Default selection: BH procedure at level %s for FDR control", alpha)
  d <- event_data("plotly_selected")
  mm <- numeric(0L)
  if (is.null(d)) {
    mm <- which(datly[["adjp.BH"]] <= alpha)
  } else {
    mm <- match(d$key, datly[["id"]])
  }
  Vbar <- posthocBySimes(datly[["p.value"]], mm, alpha)
  msg <- sprintf("%s\nAt least %s true positives among %s selected genes", msg, Vbar, length(mm))
  fdp <- round(1 - Vbar/length(mm), 2)
  msg <- sprintf("%s (FDP %s %s)", msg, ifelse(fdp==0, "=", "<="), fdp)
  msg
})


output$selection <- renderDataTable({
#output$selection <- renderTable({
  d <- event_data("plotly_selected")
  if (is.null(d)) {
    mm <- which(datly[["adjp.BH"]] <= alpha)
  } else {
    mm <- match(d$key, datly[["id"]])
  }
  sdat <- datly[mm, ]
  o <- order(sdat[["p.value"]])
  sdat[o, ]
}, options = list(pageLength = 5))
```


### 3 - Data-driven sets of hypotheses

Here we apply logistic lasso and elastic net regression in order to identify a subset of genes that can predict the "BCR/ABL" vs "NEG" status of a patient from their expression level:

```{r}
library("glmnet")
y <- colnames(dat)
x <- t(dat)
fitL <- glmnet(x, y, alpha = 1, family = "binomial")
fitE <- glmnet(x, y, alpha = 0.5, family = "binomial")
```


```{r}
plot(fitL, main = "Lasso regularization path")
```

```{r}
plot(fitE, main = "Elastic net regularization path")
```

```{r}
RSbar <- function(pred, fit) {
  beta <- fit$beta
  non0 <- colSums(beta != 0)
  ww <- max(which(non0 <= pred))
  ids <- which(beta[, ww] != 0)
  Sbar <- posthocBySimes(pval, select = ids, alpha = 0.05)
  c(R = length(ids), Sbar = Sbar) #, FDPbar = (R - Sbar)/max(R, 1) 
}
```


```{r}
npreds <- 1:50

stats <- sapply(npreds, RSbar, fitL)
stats <- data.frame(Sbar = stats["Sbar", ], R = stats["R", ])
stats$FDPbar <- (stats$R-stats$Sbar)/pmax(stats$R, 1)
statsL <- stats

stats <- sapply(npreds, RSbar, fitE)
stats <- data.frame(Sbar = stats["Sbar", ], R = stats["R", ])
stats$FDPbar <- (stats$R-stats$Sbar)/pmax(stats$R, 1)
statsE <- stats
```


We plot our obtained lower bound on the number of true positives as a function of the number of genes in the predictor:


```{r}
plot(statsE[["R"]], statsE[["Sbar"]], t = "b", 
     xlab = "Number of variables selected",
     ylab = "Lower bound on the number of true positives")
lines(statsL[["R"]], statsL[["Sbar"]], t = "b", col = 2)

legend("topleft", c("Elastic net (0.5)", "Lasso"), col = c(1,2), lty = 1, pch = 1)
```

With the same data we can plot the corresponding upper bound on the False Discovery Proportion:

```{r}
plot(stats[["R"]], stats[["FDPbar"]], t = "b", col = 2,
     xlab = "Number of variables selected by lasso",
     ylab = "Upper bound on the proportion of false positives")
lines(statsL[["R"]], statsL[["FDPbar"]], t = "b", col = 1)
legend("bottomright", c("Elastic net (0.5)", "Lasso"), col = c(1,2), lty = 1, pch = 1)
```

Importantly, all these bounds are valid *simultaneously*. 

TODO: choose the `alpha` parameter in elastic net *post hoc* using the above FDP curves!


## Session information

```{r session-info}
sessionInfo()
```
